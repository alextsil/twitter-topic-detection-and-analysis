\documentclass[12pt,svgnames]{report}
\usepackage{tikz}
\usepackage{verbatim}
\usepackage[margin=1.1in]{geometry}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage[hidelinks]{hyperref}
\usepackage{kpfonts}
\usepackage{url}
\usepackage[cc]{titlepic} 
\usepackage[explicit]{titlesec}
\usepackage{xcolor}
\usepackage{sectsty}
\usepackage{float}
\definecolor{mycolor}{HTML}{0059b3}
\definecolor{mycolor2}{HTML}{0066cc}
\sectionfont{\color{mycolor}}
\subsectionfont{\color{mycolor2}}
\subsubsectionfont{\color{mycolor2}}
\newcommand*\chapterlabel{}
\titleformat{\chapter}
{\gdef\chapterlabel{}
	\normalfont\sffamily\Huge\bfseries\scshape}
{\gdef\chapterlabel{\thechapter\ }}{0pt}
{\begin{tikzpicture}[remember picture,overlay]
	\node[yshift=-1cm] at (current page.north west) %poso paxi tha einai to mple rectangle
	{\begin{tikzpicture}[remember picture, overlay]
		\draw[fill=MidnightBlue] (0,-1) rectangle
		(\paperwidth,2cm);
		\node[anchor=west,xshift=.1\paperwidth,rectangle,
		rounded corners=15pt,inner sep=9pt,
		fill=MidnightBlue]
		{\color{white}\chapterlabel#1};
		\end{tikzpicture}
	};
\end{tikzpicture}
}
\titlespacing*{\chapter}{0pt}{50pt}{-60pt}

\title{\color{mycolor}Report on Twitter topic detection and analysis}
\titlepic{\includegraphics[width=\textwidth]{twitter3.jpg}}
\author{
Alex Tsilingiris \\
Christina Pardalidou \\
Sotiris Karapostolakis \\
}

\begin{document}
\maketitle
\begingroup
\color{mycolor}
\tableofcontents
\endgroup
\chapter{Introduction}
This technical report as detailed below describes the implementation of a project regarding twitter topic detection and analysis. The main goal was (a) the detection of emerging events from a collected dataset crawled from twitter live streaming, (b) the extraction of sentiments concerning the top emerging event and (c) specifying the users' geolocation  that were tweeting about this event. These were achieved using methodologies and techniques that are found in the literature. Hence,  \autoref{chap:method} presents the implementation details for every topic, \autoref{chap:framework} describes the framework in a more technical manner and \autoref{chap:results} depicts the results.
\chapter{Method implementation details}
\label{chap:method}
\section*{Event detection}
\subsection*{Our approach}
A combination of approaches was used for event detection: Since the dataset was static and filtered based on a query (in our case the word "trump"), terms were considered as living organisms in the given timespan, allowing us to sample a set of tweets as when each term was "most alive", while combining user reputation metrics to select a tweet from a reputable source.
\subsubsection*{Hashtags as terms}
We solely operated on hashtags on this step since they represent an idea or a topic, which in other cases would be difficult, or not as accurate, to define using Natural Language Processing and Machine Learning.
\subsubsection*{The living organism implementation}
We considered a term as most-alive at the point in which it was mostly detected in our dataset. The time window was set to 2 hours. Therefore, we had to query for the mostly found terms (using the Term Frequency statistic) and sort the results in descending order. We then generated the final that contained the original tweets with the most active terms in the dataset that were posted in the given timespan.
\subsubsection*{Selecting a tweet from the pool}
We now have a pool of tweets that might contain information about an event. The approach we followed was to assume that a user with a high amount of followers represents an influential event source  into a social community\cite{cataldi2013personalized}. We sorted (descending) the tweet pool by the number of followers the original poster has and ended up with 1 tweet that was our result for each term.
\section*{Sentiment Analysis}

\subsection*{Methodology}
For the prediction of the sentiments of the tweets gathered, we used a supervised method. In particular, we used a machine-learning technique. The tool we used for the predictions was implemented for the needs of a course in the previous semester. Since we had put a lot of effort in its implementation and it is a creation of our personal work we decided to use it for this assignment too.

\subsubsection*{Selecting tweets from the dataset}
All the tweets related to the top emerging event were isolated and written to different text files. These are all the tweets that include hashtags related to the event (e.g \#parisagreement). All the retweets  were removed. 

\subsubsection*{Sentiment Prediction}
The Opinion-Mining tool\cite{opinion} we used implements a SVM classifier that classifies the tweets into two categories: positive and negative. SVM performs very well and is considered as one of the top performing classifiers in the literature. This tool is further presented in \autoref{chap:framework}.

\section*{Location Prediction}

\subsection*{Geoinference}
Twitter provides the users the ability to infer the location either by defining the place the user is or by enabling the location, where the position of the user is more accurate. The generic approach started by gathering, through a query to our database, both users' categories. We excluded the users with minor place inference, due to the lack of information that could lead to a correct prediction. The main idea was to form a model to foresee some of those users' location.

\subsection*{Dataset Creation}
We created 4 different datasets so we can have the ability to compare the results. Initially we collected users' tweets having as a criterion a specific number (i.e 20) of the most inferenced locations. We noticed that all of them were in the USA. The second criterion was the number of users for each locality. Based on the second, we formulated the 4 different datasets. 

\subsection*{Implementation}
The methodology is based on a simple technique. For every user we calculated the frequency of every location's reference in his/her tweets. The highest number was given as a prediction. We consulted that a user is most probable to cite his/her city more frequent than any other place. The results were quite encouraging.

\chapter{Framework description}
\label{chap:framework}
The software created was split into modules to allow independent, on-demand functionality. A user can easily assemble a system from these independent modules based on their needs. The complete repository can be found in the project's Github page \cite{projectgithub}.
\subsubsection*{Getting the tweets}
To begin with, Python and Tweepy \cite{tweepy} were used to access the Twitter API as efficiently as possible. The incoming stream was filter using the keyword "trump" and was directly saved into a MongoDb instance.

\subsubsection*{Preprocessing the dataset}
A custom preprocessing module was created to combine all the required steps needed when analyzing tweets. The nltk \cite{nltk} library along with some custom regular expressions were used in order to complete the following actions: group hashtag symbols with the hashtag word, group mention symbols with the mention target, keep URLs functional and finally remove stop-words, punctuation and twitter specifics such as "via" and "rt". Finally, a script was created to convert datetime strings into MongoDb specific datetime strings so that range queries were possible.

\subsubsection*{Plotting}
The pandas \cite{pandas} library was used to resample the time and allow tweet bucketing based on a given time window. The window size used for term (hashtag) visualization was 120minutes. The results were exported directly into .json format and are presented in our website \cite{projectwebsite} using the d3js \cite{d3js} library.

\subsubsection*{Getting the tweets for sentiment analysis}
After detecting the top event that occurred during the time the tweets were crawled, the dataset was filtered in order to get only the tweets, without the retweets, containing hashtags related to the emerging event (e.g \#parisagreement, \#climatechange). These specific tweets were written to text files so they can be imported to the Opinion-Mining tool\cite{opinion} for sentiment prediction.

\subsubsection*{Geoprediction datasets}
We mentioned the use of 4 different datasets. Now, we are going to explain the way of forming the datasets. The first step was to calculate the most referenced places. From those we selected the top-20 cities for the first, third and fourth dataset. We excluded places such as states or countries. The second step was the selection of a specific number of unique users for each dataset. The numbers were 20, 15, 25, 30 for each corresponding dataset. Every time we picked the 200 latest tweets for each user. The algorithm was applied in every dataset.

\subsubsection{Opinion-Mining tool}
The tool uses a custom lexicon implementation that is shown in figure \ref{fig:opinion}. The training set used includes a total of 25k labeled reviews taken from the published dataset of IMDb \cite{imdb}. Half of them are positive and the other half are negative.
\begin{figure}[h]
	\centering
	\includegraphics{opinion}
	\caption{Depiction of the diagram of the lexicon.}
	\label{fig:opinion}
\end{figure}

\subsubsection{Preprocessing process}
\begin{itemize}
	\item Punctuation removal, tokenization.
	\item Stemming (non-English terms are deleted).
	\item Terms frequencies.
	\item TF/IDF Vectors.
\end{itemize}

\subsubsection{Prediction}
The model uses a SVM classifier with SGD tuning for prediction. The vectors are defined with TF: log normalized and IDF: smooth normalized. The output is a text file that has in each row the name of the input text file and the output of the prediction for this file i.e 1.0 for positive and 0.0 for negative tweets.

\subsubsection*{Geoprediction Approach}
\begin{enumerate}
	\item Tokenization: Collect the tweets of every user and create a list of tokens for each user.
	\item Stopwords removal.
	\item Frequencies calculation: The most frequent word is given as a prediction. The algorithm also counts words that are included in hashtags (e.g paris in \#prayforparis).
\end{enumerate}

\chapter{Results}
\label{chap:results}
\section*{Event detection}
Some top ranked terms (hashtags), their occurrences and the tweet that qualified as an event by our system will be presented in table \ref{tab:tftags}

\begin{table}[h!]
	\centering

	\begin{tabular}{ccc}
		\toprule
		Term & Frequency\\
		\midrule
		\#parisagreement & 10442\\
		\#covfefe & 7085\\
		\#trumprussia & 4229\\
		\#marchfortruth & 973\\
		\bottomrule
	\end{tabular}
	\caption{Terms and occurrences}
\label{tab:tftags}
\end{table}


A graph showing the top ranked term's occurrence over time can be seen in the following figure:\\
\\
\includegraphics[scale=0.63]{hashparisplot.png}
\\

\newpage
The tweet that our system determined as an event can be seen in the following figure:
\begin{figure}[h]
\centering
\includegraphics[scale=1]{hashparistweet.png}
\end{figure}
\\
Additional results can be found in the project's website \cite{projectwebsite}.

\section*{Sentiment Analysis}
The predictions of the sentiments of the tweets are presented using a pie chart in \ref{fig:pie_chart}. The number of tweets that corresponds to each sentiment is presented in Table \ref{tab:sentis}.

\begin{table}[H]
	\centering	
	\begin{tabular}{ccc}
		\toprule
		Opinion & Number of tweets\\
		\midrule
		Negative & 1501\\
		Positive & 1183\\		
		\bottomrule
		Total & 2684
	\end{tabular}
	\caption{Predictions of the tweets}
	\label{tab:sentis}
\end{table}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{pie_chart}
	\caption{Percentages of negative and positive tweets.}
	\label{fig:pie_chart}
\end{figure}

\section*{Location Prediction}
The chosen representation type for the results of Geoprediction experiments is the bar plot. The first chart represents the top referenced locations. The second one the top 20 locations for our experiments. The last bar chart depicts the accuracies of the models for every dataset. The metric used for the evaluation of our implementation is accuracy. The highest accuracy was achieved by the second dataset, in which the number of users and the number of cities is smaller (i.e 15 for cities and 15 for users/city) than in any other case.

\begin{figure}[H]
	\centering
	\includegraphics[scale = 0.5]{toplocations.png}
	\caption{Most frequent locations}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[scale = 0.5]{top20cities.png}
	\caption{Top 20 selected locations}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[scale = 0.5]{accuracies.png}
	\caption{Accuracies of the datasets}
\end{figure}

\bibliographystyle{abbrv}
\bibliography{sigproc}
\addcontentsline{toc}{chapter}{Bibliography}
\end{document}